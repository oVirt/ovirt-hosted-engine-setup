---
- name: Create target Hosted Engine VM
  hosts: localhost
  connection: local
  tasks:
  - name: Register the engine FQDN as a host
    add_host:
      name: "{{ FQDN }}"
      groups: engine
      ansible_connection: smart
      ansible_ssh_extra_args: -o StrictHostKeyChecking=no
      ansible_ssh_pass: "{{ APPLIANCE_PASSWORD }}"
      ansible_user: root
  - include_tasks: auth_sso.yml
  - name: Get local VM IP
    shell: virsh -r net-dhcp-leases default | grep -i {{ VM_MAC_ADDR }} | awk '{ print $5 }' | cut -f1 -d'/'
    register: local_vm_ip
    changed_when: True
  - debug: var=local_vm_ip
  - name: Fetch host facts
    ovirt_hosts_facts:
      pattern: name={{ HOST_NAME }} status=up
      auth: "{{ ovirt_auth }}"
    register: host_result
    until: host_result|succeeded and host_result.ansible_facts.ovirt_hosts|length >= 1
    retries: 50
    delay: 10
  - debug: var=host_result
  - name: Fetch Cluster ID
    set_fact: cluster_id="{{ host_result.ansible_facts.ovirt_hosts[0].cluster.id }}"
  - name: Fetch Cluster facts
    ovirt_cluster_facts:
      auth: "{{ ovirt_auth }}"
    register: cluster_facts
  - debug: var=cluster_facts
  - name: Fetch Datacenter facts
    ovirt_datacenter_facts:
      auth: "{{ ovirt_auth }}"
    register: datacenter_facts
  - debug: var=datacenter_facts
  - name: Fetch Cluster name
    set_fact: cluster_name={{ ovirt_clusters|json_query("[?id=='" + cluster_id + "'].name")|first }}
  - name: Fetch Datacenter ID
    set_fact: datacenter_id={{ ovirt_clusters|json_query("[?id=='" + cluster_id + "'].data_center.id")|first }}
  - name: Fetch Datacenter name
    set_fact: datacenter_name={{ ovirt_datacenters|json_query("[?id=='" + datacenter_id + "'].name")|first }}
    # TODO: properly get it from the engine via REST API once properly exposed
    # see: https://bugzilla.redhat.com/show_bug.cgi?id=1542531
  - name: Get Cluster CPU model
    set_fact: cluster_cpu_model={{ ovirt_clusters|json_query("[?id=='" + cluster_id + "'].cpu.type")
      |first
      |trim
      |replace('Intel ', '')|replace('AMD ', '')|replace('IBM ', '')
      |replace(' Family', '')
      |replace('Opteron G', 'Opteron_G')
      |replace(' ', '-') }}
  - debug: var=cluster_cpu_model
  - name: Get storage domain details
    ovirt_storage_domains_facts:
      pattern: name={{ STORAGE_DOMAIN_NAME }} and datacenter={{ datacenter_name }}
      auth: "{{ ovirt_auth }}"
    register: storage_domain_details
  - debug: var=storage_domain_details
  - name: Add HE disks
    ovirt_disk:
      name: "{{ item.name }}"
      size: "{{ item.size }}"
      format: "{{ item.format }}"
      sparse: "{{ item.sparse }}"
      description: "{{ item.description }}"
      interface: virtio
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      wait: True
      timeout: 600
      auth: "{{ ovirt_auth }}"
    with_items:
      - { name: 'he_sanlock', description: 'Hosted-Engine sanlock disk', size: '1GiB', format: 'raw', sparse: False }
      - { name: 'HostedEngineConfigurationImage', description: 'Hosted-Engine configuration disk', size: '1GiB', format: 'raw', sparse: False }
      - { name: 'he_metadata', description: 'Hosted-Engine metadata disk', size: '1GiB', format: 'raw', sparse: False }
      - { name: 'he_virtio_disk', description: 'Hosted-Engine disk', size: "{{ DISK_SIZE }}GiB", format: 'raw', sparse: "{{ False if DOMAIN_TYPE == 'fc' or DOMAIN_TYPE == 'iscsi' else True }}" }
    register: add_disks
  - name: Register disk details
    set_fact:
      he_sanlock_disk_details: "{{ add_disks.results[0] }}"
      he_conf_disk_details: "{{ add_disks.results[1] }}"
      he_metadata_disk_details: "{{ add_disks.results[2] }}"
      he_virtio_disk_details: "{{ add_disks.results[3] }}"
  - debug: var=add_disks
  - name: Add VM
    ovirt_vms:
      state: stopped
      cluster: "{{ cluster_name }}"
      name: "{{ VM_NAME }}"
      description: 'Hosted Engine Virtual Machine'
      memory: "{{ MEM_SIZE }}Mib"
      cpu_cores: "{{ VCPUS }}"
      cpu_sockets: 1
      serial_console: True
      operating_system: rhel_7x64
      type: server
      high_availability_priority: 1
      delete_protected: True
#      timezone: "{{ TIME_ZONE }}" # TODO: fix with the right parameter syntax
      storage_domain: "{{ STORAGE_DOMAIN_NAME }}"
      disks:
      - name: he_virtio_disk
      nics:
      - name: vnet0
        profile_name: "{{ BRIDGE }}"
        interface: virtio
        mac_address: "{{ VM_MAC_ADDR }}"
      auth: "{{ ovirt_auth }}"
    register: he_vm_details
  - debug: var=he_vm_details
  - name: Register external local VM uuid
    shell: virsh -r domuuid {{ VM_NAME }}Local | head -1
    register: external_local_vm_uuid
    changed_when: True
  - debug: var=external_local_vm_uuid
- hosts: engine
  tasks:
  - name: Find configuration file for SCL PostgreSQL
    shell: ls -vr1 /etc/ovirt-engine/engine.conf.d/*-scl-postgres-*.conf
    register: scl_conf_flist
    changed_when: True
  - debug: var=scl_conf_flist
  - name: Check SCL PostgreSQL value
    command: grep sclenv {{ scl_conf_flist.stdout_lines|first }}
    register: scl_conf
    changed_when: True
  - debug: var=scl_conf
  - name: Update target VM details at DB level
    command: scl enable {{ scl_conf.stdout_lines[0].split('=')[1].replace('\"','') }} -- psql -d engine -c "UPDATE vm_static SET {{ item.field }}={{ item.value }} WHERE vm_guid='{{ hostvars['localhost']['he_vm_details']['vm']['id'] }}'"
    become: true
    become_user: postgres
    become_method: sudo
    changed_when: True
    register: db_vm_update
    with_items:
      - { field: 'origin', value: 6 }
  - debug: var=db_vm_update
  - name: Insert Hosted Engine configuration disk uuid into Engine database
    command: scl enable {{ scl_conf.stdout_lines[0].split('=')[1].replace('\"','') }} -- psql -d engine -c "UPDATE vdc_options SET option_value='{{ hostvars['localhost']['he_conf_disk_details']['disk']['id'] }}' WHERE option_name='HostedEngineConfigurationImageGuid' AND version='general'"
    become: true
    become_user: postgres
    become_method: sudo
    changed_when: True
    register: db_conf_update
  - debug: var=db_conf_update
  - name: Disable IPv6
    lineinfile:
      path: /etc/sysctl.conf
      line: 'net.ipv6.conf.all.disable_ipv6=1'
  - name: Reload sysctl
    command: sysctl -p
    changed_when: True
- hosts: localhost
  connection: local
  tasks:
  - name: Trigger hosted engine OVF update
    ovirt_vms:
      id: "{{ he_vm_details.vm.id }}"
      description: "Hosted engine VM"
      auth: "{{ ovirt_auth }}"
  - name: Wait until OVF update finishes
    ovirt_storage_domains_facts:
      auth: "{{ ovirt_auth }}"
      fetch_nested: True
      nested_attributes:
        - name
        - image_id
        - id
      pattern: "name={{ STORAGE_DOMAIN_NAME }}"
    retries: 12
    delay: 10
    until: "ovirt_storage_domains[0].disks | selectattr('name', 'match', '^OVF_STORE$') | list"
  - name: Parse OVF_STORE disk list
    set_fact:
      ovf_store_disks: "{{ ovirt_storage_domains[0].disks | selectattr('name', 'match', '^OVF_STORE$') | list }}"
  - debug: var=ovf_store_disks
  - name: Check OVF_STORE volume status
    command: vdsm-client Volume getInfo storagepoolID={{ datacenter_id }} storagedomainID={{ storage_domain_details.ansible_facts.ovirt_storage_domains[0].id }} imageID={{ item.id|first }} volumeID={{ item.image_id  }}
    changed_when: True
    register: ovf_store_status
    retries: 12
    delay: 10
    until: ovf_store_status.rc == 0 and ovf_store_status.stdout|from_json|json_query('status') == 'OK' and ovf_store_status.stdout|from_json|json_query('description')|from_json|json_query('Updated')==true
    with_items: "{{ ovf_store_disks }}"
  - debug: var=ovf_store_status
  - name: Prepare images
    command: vdsm-client Image prepare storagepoolID={{ datacenter_id }} storagedomainID={{ storage_domain_details.ansible_facts.ovirt_storage_domains[0].id }} imageID={{ item.disk.id }} volumeID={{ item.disk.image_id  }}
    with_items:
      - "{{ he_virtio_disk_details }}"
      - "{{ he_conf_disk_details }}"
      - "{{ he_metadata_disk_details }}"
      - "{{ he_sanlock_disk_details }}"
    register: prepareimage_results
    changed_when: True
  - debug: var=prepareimage_results
  - name: Fetch Hosted Engine configuration disk path
    set_fact: he_conf_disk_path={{ (prepareimage_results.results|json_query("[?item.id=='" + he_conf_disk_details.id + "'].stdout")|first|from_json).path }}
  - name: Fetch Hosted Engine virtio disk path
    set_fact: he_virtio_disk_path={{ (prepareimage_results.results|json_query("[?item.id=='" + he_virtio_disk_details.id + "'].stdout")|first|from_json).path }}
  - name: Fetch Hosted Engine virtio metadata path
    set_fact: he_metadata_disk_path={{ (prepareimage_results.results|json_query("[?item.id=='" + he_metadata_disk_details.id + "'].stdout")|first|from_json).path }}
  - debug: var=he_conf_disk_path
  - debug: var=he_virtio_disk_path
  - debug: var=he_metadata_disk_path
  - name: Shutdown local VM
    virt:
      name: "{{ VM_NAME }}Local"
      command: shutdown
      uri: 'qemu+tls://{{ HOST_ADDRESS }}/system'
  - name: Undefine local VM
    virt:
      name: "{{ VM_NAME }}Local"
      command: undefine
      uri: 'qemu+tls://{{ HOST_ADDRESS }}/system'
  - name: Detect spmId
    command: vdsm-client StoragePool getSpmStatus storagepoolID={{ datacenter_id }}
    register: get_spm_id_out
    changed_when: True
  - debug: var=get_spm_id_out
  - name: Parse spmId
    set_fact: spmId="{{ get_spm_id_out.stdout|from_json|json_query('spmId') }}"
  - debug: var=spmId
  - name: Detect ovirt-hosted-engine-ha version
    command: python -c 'from ovirt_hosted_engine_ha.agent import constants as agentconst; print(agentconst.PACKAGE_VERSION)'
    register: ha_version_out
    changed_when: True
  - name: Set ha_version
    set_fact: ha_version="{{ ha_version_out.stdout_lines|first }}"
  - debug: var=ha_version
  - name: Create configuration templates
    template:
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
    with_items:
# TODO: properly get {{ EMULATED_MACHINE }} from the engine via REST API once
# properly exposed. See: https://bugzilla.redhat.com/show_bug.cgi?id=1542531
      - { src: templates/vm.conf.j2, dest: "{{ LOCAL_VM_DIR }}/vm.conf" }
      - { src: templates/broker.conf.j2, dest: "{{ LOCAL_VM_DIR }}/broker.conf" }
      - { src: templates/version.j2, dest: "{{ LOCAL_VM_DIR }}/version" }
      - { src: templates/fhanswers.conf.j2, dest: "{{ LOCAL_VM_DIR }}/fhanswers.conf" }
      - { src: templates/hosted-engine.conf.j2, dest: "{{ LOCAL_VM_DIR }}/hosted-engine.conf" }
  - name: Create configuration archive
    command: tar --record-size=20480 -cvf {{ he_conf_disk_details.disk.image_id }} vm.conf broker.conf version fhanswers.conf hosted-engine.conf
    args:
      chdir: "{{ LOCAL_VM_DIR }}"
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Create ovirt-hosted-engine-ha run directory
    file:
      path: /var/run/ovirt-hosted-engine-ha
      state: directory
  - name: Copy configuration files to the right location on host
    copy:
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
    with_items:
      - { src: "{{ LOCAL_VM_DIR }}/vm.conf", dest: /var/run/ovirt-hosted-engine-ha }
      - { src: "{{ LOCAL_VM_DIR }}/hosted-engine.conf", dest: /etc/ovirt-hosted-engine/ }
  - name: Copy configuration archive to storage
    command: dd bs=20480 count=1 oflag=direct if="{{ LOCAL_VM_DIR }}/{{ he_conf_disk_details.disk.image_id }}" of="{{ he_conf_disk_path }}"
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Initialize metadata volume
    command: dd bs=1M count=1024 oflag=direct if=/dev/zero of="{{ he_metadata_disk_path }}"
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Find the local appliance image
    find:
      paths: "{{ LOCAL_VM_DIR }}/images"
      recurse: true
      patterns: ^.*.(?<!meta)$
      use_regex: true
    register: app_img
  - debug: var=app_img
  - name: Copy local VM disk to shared storage
    command: qemu-img convert -n -O raw {{ app_img.files[0].path }} {{ he_virtio_disk_path }}
    become_user: vdsm
    become_method: sudo
    changed_when: True
    tags: [ 'skip_ansible_lint' ]
  - name: Generate DHCP network configuration for the engine VM
    template:
      src: templates/ifcfg-eth0-dhcp.j2
      dest: "{{ LOCAL_VM_DIR }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: VM_IP_ADDR is none
  - name: Generate static network configuration for the engine VM
    template:
      src: templates/ifcfg-eth0-static.j2
      dest: "{{ LOCAL_VM_DIR }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: VM_IP_ADDR is not none
  - name: Inject network configuration with guestfish
    shell: LIBGUESTFS_BACKEND=direct guestfish -a {{ he_virtio_disk_path }} --rw -i copy-in "{{ LOCAL_VM_DIR }}/ifcfg-eth0" /etc/sysconfig/network-scripts {{ ":" }} selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts /etc/sysconfig/network-scripts/ifcfg-eth0 force{{ ":" }}true
    become_user: vdsm
    become_method: sudo
    tags: [ 'skip_ansible_lint' ]
  - name: Extract /etc/hosts from the Hosted Engine VM
    shell: LIBGUESTFS_BACKEND=direct virt-copy-out -a {{ he_virtio_disk_path }} /etc/hosts "{{ LOCAL_VM_DIR }}"
    become_user: vdsm
    become_method: sudo
    when: not VM_ETC_HOSTS
    tags: [ 'skip_ansible_lint' ]
  - name: Clean /etc/hosts for the Hosted Engine VM
    lineinfile:
      dest: "{{ LOCAL_VM_DIR }}/hosts"
      line: "{{ HOST_IP }} {{ HOST_ADDRESS }}"
      state: absent
    when: not VM_ETC_HOSTS
  - name: Copy /etc/hosts back to the Hosted Engine VM
    shell: LIBGUESTFS_BACKEND=direct virt-copy-in -a {{ he_virtio_disk_path }} "{{ LOCAL_VM_DIR }}"/hosts /etc
    become_user: vdsm
    become_method: sudo
    when: not VM_ETC_HOSTS
    tags: [ 'skip_ansible_lint' ]
  - name: Clean /etc/hosts on the host
    lineinfile:
      dest: /etc/hosts
      line: "{{ local_vm_ip.stdout_lines[0] }} {{ FQDN }}"
      state: absent
  - name: Add an entry in /etc/hosts for the target VM
    lineinfile:
      dest: /etc/hosts
      line: "{{ VM_IP_ADDR }} {{ FQDN }}"
      state: present
    when: VM_ETC_HOSTS and VM_IP_ADDR is not none
  - name: Start ovirt-ha-broker service on the host
    service:
      name: ovirt-ha-broker
      state: started
      enabled: true
  - name: Initialize lockspace volume
    command: hosted-engine --reinitialize-lockspace --force
    register: result
    until: result.rc == 0
    retries: 5
    delay: 10
    changed_when: True
  - debug: var=result
  - name: Start ovirt-ha-agent service on the host
    service:
      name: ovirt-ha-agent
      state: started
      enabled: true
  - name: Wait for the engine to come up on the target VM
    command: hosted-engine --vm-status --json
    register: health_result
    until: health_result.rc == 0 and 'health' in health_result.stdout and health_result.stdout|from_json|json_query('*."engine-status"."health"')|first=="good"
    retries: 120
    delay: 5
    changed_when: True
  - debug: var=health_result
  - include_tasks: auth_sso.yml
  # Workaround for https://bugzilla.redhat.com/1540107
  # the engine fails deleting a VM if its status in the engine DB
  # is not up to date.
  - name: Check for the local bootstrap VM
    ovirt_vms_facts:
      pattern: id="{{ external_local_vm_uuid.stdout_lines|first }}"
      auth: "{{ ovirt_auth }}"
    register: local_vm_f
  - name: Remove the bootstrap local VM
    block:
      - name: Make the engine aware that the external VM is stopped
        ignore_errors: yes
        ovirt_vms:
          state: stopped
          id: "{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: vmstop_result
      - debug: var=vmstop_result
      - name: Wait for the local bootstrap VM to be down at engine eyes
        ovirt_vms_facts:
          pattern: id="{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: local_vm_status
        until: local_vm_status.ansible_facts.ovirt_vms[0].status == "down"
        retries: 24
        delay: 5
      - debug: var=local_vm_status
      - name: Remove bootstrap external VM from the engine
        ovirt_vms:
          state: absent
          id: "{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: vmremove_result
      - debug: var=vmremove_result
    when: local_vm_f.ansible_facts.ovirt_vms|length > 0

  - name: Include custom tasks for after setup customization
    include_tasks: "{{ item }}"
    with_fileglob: "hooks/after_setup/*.yml"
    register: after_setup_results
  - debug: var=after_setup_results
...
